{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0Lyiyor3cArM/f8sgIp65",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leomercanti/Course_Advanced_Investing_with_AI/blob/main/Module_3_Reinforcement_Learning_in_Financial_Markets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Module 3: Reinforcement Learning in Financial Markets**\n",
        "\n",
        "\n",
        "If you havent checked our previous modules yet, you can find them on the links below:\n",
        "- [Module 1](https://colab.research.google.com/drive/15iRO6g-AyE2vGtdodh4xZ5RLmAcPcNV_)\n",
        "- [Module 2](https://colab.research.google.com/drive/1Xr_athUWZH3iKZ6ubzvPnEAN9lPnee-1)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Learning Goals:**\n",
        "- Understand the foundations of Reinforcement Learning and how it differs from other machine learning paradigms.\n",
        "- Implement a basic Q-Learning algorithm to model stock trading decisions.\n",
        "- Explore advanced RL techniques like Deep Q-Networks (DQN) and policy-based methods (e.g., PPO) for more complex environments.\n",
        "- Learn to backtest RL-driven trading strategies to evaluate performance.\n"
      ],
      "metadata": {
        "id": "WOPK7swfVHRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Core Readings and Resources\n",
        "\n",
        "- **Textbook:** \"Reinforcement Learning: An Introduction\" by Sutton and Barto\n",
        "\n",
        "  - Chapter 4: Dynamic Programming and Q-Learning – Provides the fundamental understanding of RL, rewards, and policies.\n",
        "\n",
        "- **Research Papers:**\n",
        "\n",
        "  - \"Deep Reinforcement Learning in Financial Trading Systems\": A practical approach to applying DQN to trading.\n",
        "  - “Reinforcement Learning for Portfolio Management”: Shows how RL can be applied to optimizing portfolios over time.\n",
        "\n",
        "- **Optional:**\n",
        "  - “Trust Region Policy Optimization” (Schulman et al.) – Essential for understanding the underlying mechanics of policy-based optimization algorithms like PPO."
      ],
      "metadata": {
        "id": "EAEco8fwWDy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Key Topics Overview\n",
        "\n",
        "**Q-Learning for Trading**\n",
        "\n",
        "- **Why Q-Learning?** Q-Learning is a fundamental RL algorithm where the agent learns to maximize its rewards through interactions with the environment. It builds a Q-Table to represent the value of taking each action in each state and uses these values to make trading decisions (e.g., buy, sell, hold). Q-Learning is a good introduction to RL in trading.\n",
        "\n",
        "- **Main Concepts:**\n",
        "\n",
        "  - States: Represent the financial environment (e.g., current stock prices, technical indicators).\n",
        "  - Actions: Possible decisions (e.g., buy, sell, hold).\n",
        "  - Rewards: Feedback received based on the outcome of an action (e.g., profit/loss from trade).\n",
        "  - Policy: The agent’s strategy, derived from the Q-Table, which determines which action to take in each state.\n",
        "\n",
        "- **Use Case:** Building a Q-Learning-based agent that learns to trade a stock based on historical price data.\n",
        "\n",
        "- **Hands-On Example:** Implementing Q-Learning for Trading"
      ],
      "metadata": {
        "id": "2KhNq3kTW6Yq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqQTpzjdVAUZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download historical data (Apple stock for this example)\n",
        "data = yf.download(\"AAPL\", start=\"2022-01-01\", end=\"2024-09-01\")\n",
        "prices = data['Close'].values"
      ],
      "metadata": {
        "id": "V9lQwtsrXhHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Q-Learning parameters\n",
        "n_actions = 3  # Buy, Hold, Sell\n",
        "n_states = len(prices)  # One state per price point\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration rate"
      ],
      "metadata": {
        "id": "6NKuPOQwXkPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Q-Table with zeros\n",
        "Q_table = np.zeros((n_states, n_actions))"
      ],
      "metadata": {
        "id": "vHw2r51FXnT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward function (Profit/Loss)\n",
        "def get_reward(action, state, next_state):\n",
        "    if action == 0:  # Buy\n",
        "        return next_state - state\n",
        "    elif action == 2:  # Sell\n",
        "        return state - next_state\n",
        "    return 0  # Hold"
      ],
      "metadata": {
        "id": "Jb__ouiGXp5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning algorithm\n",
        "def q_learning(episodes=1000):\n",
        "    for episode in range(episodes):\n",
        "        state_idx = np.random.randint(0, n_states - 1)\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Choose action: Exploration vs. Exploitation\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.randint(0, n_actions)\n",
        "            else:\n",
        "                action = np.argmax(Q_table[state_idx])\n",
        "\n",
        "            # Take action and observe next state and reward\n",
        "            next_state_idx = (state_idx + 1) % n_states\n",
        "            reward = get_reward(action, prices[state_idx], prices[next_state_idx])\n",
        "\n",
        "            # Update Q-Table\n",
        "            best_next_action = np.argmax(Q_table[next_state_idx])\n",
        "            Q_table[state_idx, action] = Q_table[state_idx, action] + alpha * (\n",
        "                reward + gamma * Q_table[next_state_idx, best_next_action] - Q_table[state_idx, action])\n",
        "\n",
        "            state_idx = next_state_idx\n",
        "            if state_idx == n_states - 1:\n",
        "                done = True"
      ],
      "metadata": {
        "id": "1cAAp_M6XtiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Q-Learning agent\n",
        "q_learning()"
      ],
      "metadata": {
        "id": "6wau1eQgXyVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Q-values\n",
        "plt.plot(Q_table)\n",
        "plt.title('Q-Values for Buy, Hold, and Sell actions over time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ewSfEuBWX0Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Expected Outcome:** The Q-Learning agent will learn to make decisions (buy/sell/hold) based on historical stock prices, maximizing profit over time. You'll visualize how Q-values evolve for each action at different states."
      ],
      "metadata": {
        "id": "7ji8XdTJX2nX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Q-Networks (DQN) for Trading**\n",
        "\n",
        "- **Why DQN?** In environments with large or continuous state spaces (e.g., complex market conditions), Q-tables become impractical. Deep Q-Networks (DQN) use a neural network to approximate the Q-function, enabling the agent to handle larger environments and more complex decision-making.\n",
        "\n",
        "- **Main Concepts:**\n",
        "\n",
        "  - Neural Network Q-Function: Instead of storing Q-values in a table, a neural network estimates the Q-values for each action based on the state.\n",
        "  - Experience Replay: Stores past experiences in a buffer and samples from this buffer during training, improving stability.\n",
        "  - Target Network: A separate neural network that updates less frequently, preventing oscillations in Q-value updates.\n",
        "\n",
        "- **Use Case:** Using DQN to make trading decisions in a more dynamic and complex financial environment."
      ],
      "metadata": {
        "id": "FPW3x0SVYCdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ],
      "metadata": {
        "id": "nYgHM14yl4ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DQN network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "LIPSmTN8rubc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize DQN parameters\n",
        "state_size = 1  # Using stock prices as states\n",
        "action_size = 3  # Buy, Hold, Sell\n",
        "model = DQN(state_size, action_size)\n",
        "target_model = DQN(state_size, action_size)  # Target network\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "O_2BpCn1r0Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replay buffer for experience replay\n",
        "replay_buffer = []\n",
        "batch_size = 32\n",
        "gamma = 0.95\n",
        "epsilon = 0.1"
      ],
      "metadata": {
        "id": "QhhE0lSpr5RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get rewards (you need to define this)\n",
        "def get_reward(action, current_price, next_price):\n",
        "    # Simple reward: profit/loss based on the action taken\n",
        "    if action == 0:  # Buy\n",
        "        return next_price - current_price\n",
        "    elif action == 1:  # Hold\n",
        "        return 0  # No profit or loss\n",
        "    elif action == 2:  # Sell\n",
        "        return current_price - next_price\n",
        "    return 0"
      ],
      "metadata": {
        "id": "RO2bjfgzr8l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTANT: This block of code might take a very long time to run.\n",
        "# It is recommended to only read it.\n",
        "\n",
        "# Train the DQN\n",
        "def train_dqn(episodes=50):\n",
        "    for episode in range(episodes):\n",
        "        state = torch.FloatTensor([[prices[0]]])  # Initial state\n",
        "        done = False\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice([0, 1, 2])  # Explore\n",
        "            else:\n",
        "                q_values = model(state)\n",
        "                action = torch.argmax(q_values).item()  # Exploit\n",
        "\n",
        "            # Take action, observe next state and reward\n",
        "            next_state = torch.FloatTensor([[prices[(episode + 1) % len(prices)]]])\n",
        "            reward = get_reward(action, prices[episode], prices[(episode + 1) % len(prices)])\n",
        "\n",
        "            # Store experience in replay buffer\n",
        "            replay_buffer.append((state, action, reward, next_state))\n",
        "\n",
        "            # Experience replay training\n",
        "            if len(replay_buffer) > batch_size:\n",
        "                batch = random.sample(replay_buffer, batch_size)\n",
        "                for s, a, r, ns in batch:\n",
        "                    # Get Q-values for the current state\n",
        "                    q_values = model(s)\n",
        "                    current_q = q_values.squeeze()[a]\n",
        "\n",
        "                    # Get the target Q-value from the target model\n",
        "                    with torch.no_grad():\n",
        "                        target_q = r + gamma * torch.max(target_model(ns))\n",
        "\n",
        "                    # Calculate loss and update model\n",
        "                    loss = loss_fn(current_q, target_q)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            state = next_state\n",
        "            if (episode + 1) % len(prices) == 0:\n",
        "                done = True\n",
        "\n",
        "train_dqn()"
      ],
      "metadata": {
        "id": "sZ1w3CEOr_k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Policy-Based Methods: Proximal Policy Optimization (PPO)**\n",
        "\n",
        "- **Why PPO?** Policy-based methods differ from Q-learning by directly optimizing the policy (i.e., the action-selection strategy) instead of estimating Q-values. PPO is a popular policy-gradient method that ensures stable and efficient learning, especially in continuous action spaces.\n",
        "\n",
        "- **Main Concepts:**\n",
        "\n",
        "  - **Policy Optimization:** Instead of learning Q-values, PPO optimizes the probability distribution over actions.\n",
        "  - **Clipping:** PPO clips policy updates to prevent large changes, ensuring stability.\n",
        "  - **Advantages:** Suitable for tasks with continuous action spaces or where Q-learning methods struggle to converge.\n",
        "  - **Use Case:** Applying PPO to optimize portfolio allocation over multiple assets."
      ],
      "metadata": {
        "id": "vNCT1NFqmc6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Advanced Concepts: Model-Free vs. Model-Based RL\n",
        "\n",
        "**Model-Free RL:**\n",
        "- **Advantages:** Simpler and widely used in finance. No need for a model of the environment.\n",
        "- **Disadvantages:** Requires a lot of data for training and can be sample-inefficient.\n",
        "\n",
        "**Model-Based RL:**\n",
        "- **Advantages:** More efficient in terms of data usage. Uses a model of the environment to simulate future states.\n",
        "- **Disadvantages:** More complex and harder to implement in unpredictable environments like financial markets."
      ],
      "metadata": {
        "id": "hOHLqDCGdDwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 End of Module Assignments and Practice (Optional)\n",
        "\n",
        "- **Assignment 1:** Implement a Q-Learning algorithm for stock trading. Track your agent's learning progress by visualizing Q-values for different actions over time.\n",
        "\n",
        "- **Assignment 2:** Build a DQN agent for trading and optimize its hyperparameters (e.g., learning rate, epsilon decay). Test the model's performance on a different stock dataset and compare results with the Q-Learning approach.\n",
        "\n",
        "- **Bonus:** Experiment with PPO for portfolio optimization. Define your portfolio and apply PPO to determine optimal asset allocations over time."
      ],
      "metadata": {
        "id": "MN80UMBCdeNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the end of **Module 3**, you should be comfortable implementing **reinforcement learning algorithms**, such as **Q-Learning** and **Deep Q-Networks (DQN)**, to develop intelligent agents that make optimal trading decisions. Additionally, you’ve gained an understanding of policy-based methods like **Proximal Policy Optimization (PPO)** for portfolio management.\n",
        "\n",
        "These techniques allow you to model complex financial environments and continuously improve strategies through trial and error.\n",
        "\n",
        "With these skills, you’re now equipped to tackle fully automated trading systems and explore cutting-edge applications of AI in finance in the next phase of the program."
      ],
      "metadata": {
        "id": "dWFqdCEudsjd"
      }
    }
  ]
}